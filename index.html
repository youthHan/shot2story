<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos">
  <meta name="keywords"
    content="Multi-shot videos, Video story, Video captioning, Video summarization, VLM, LLM, Large Language Models, Multi-modal learning, Benchmark, Dataset, Instruction tuning, Shot2Story">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .teaser-section {
        background-color: #f0f0f0; /* Light grey background for contrast */
    }

    .news-item {
      background-color: #f0f8ff; /* Light blue background for contrast */
      border-left: 5px solid #007bff; /* Blue border for emphasis */
      padding: 10px 20px; /* Padding for spacing */
      margin: 10px 0; /* Margin for separation between items */
      font-family: Arial, sans-serif; /* Font styling */
    }

    .news-bug-item {
      background-color: #fffcf0; /* Light blue background for contrast */
      border-left: 5px solid #f7b605; /* Blue border for emphasis */
      padding: 10px 20px; /* Padding for spacing */
      margin: 10px 0; /* Margin for separation between items */
      font-family: Arial, sans-serif; /* Font styling */
    }

    .news-icon {
        color: #ff4500; /* Orange color for the icon */
        font-size: 20px; /* Larger size for the icon */
    }

    .news-title {
        color: #007bff; /* Blue color for the title */
        font-weight: bold; /* Bold font for the title */
        font-size: 18px; /* Font size for the title */
    }

    .news-bug-title {
        color: #f7b605; /* Blue color for the title */
        font-weight: bold; /* Bold font for the title */
        font-size: 18px; /* Font size for the title */
    }

    .news-content {
        color: #333333; /* Dark grey for content text */
        font-size: 14px; /* Font size for content */
        margin-top: 10px; /* Space above the content */
    }
  </style>

</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://mingfei.info">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>
      </div>
    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Shot2Story</h1>
            <h1 class="title is-3 publication-title">A New Benchmark for Comprehensive Understanding of Multi-shot Videos</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://mingfei.info">Mingfei Han<sup>1,2,3*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang<sup>2*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.xiaojun.ai/">Xiaojun Chang<sup>1,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://hengcv.github.io/">Heng Wang<sup>2</sup></a>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution</span>
            </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>ReLER, AAII, University of Technology Sydney,</span>
              <span class="author-block"><sup>2</sup>Bytedance Inc.,</span>
              <span class="author-block"><sup>3</sup>Data61, CSIRO,</span>
              <span class="author-block"><sup>4</sup>Department of Computer Vision, MBZUAI</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://mingfei.info/files/paper_shot2story20k.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.10300"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Link to Hugging Face Space -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/mhan/Shot2Story"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf-logo.svg" alt="Huggingface logo" width="20" height="20">
                    </span>
                    <span>Demo</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/bytedance/Shot2Story/tree/master/code"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=uhgUrxo1r80"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Link to Hugging Face Code -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/your-code-link"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-code"></i>
                  </span>
                  <span>Hugging Face Code</span>
                </a>
              </span> -->

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container">
      <div class="news-bug-item">
          <span class="news-icon" style="color: #4CAF50;">ðŸŒŸ</span>
          <span class="news-bug-title">Data instruction: Please find our instructions for using and downloading data <a href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md" style="color: #f7b605;"><u>here</u></a>.</span>
          <!-- <br> -->
          <!-- <span style="font-size: 14px;">Detailed and grounded video summaries are powerful! Please check our demo for SUM-shot model. Chat-SUM-shot model is on the way!</span> -->
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">ðŸš€</span>
          <span class="news-title">Demo Release: <a href="https://huggingface.co/spaces/mhan/Shot2Story" style="color: #007bff;"><u>SUM-shot model</u></a></span>
          <br>
          <span style="font-size: 14px;">Detailed and grounded video summaries are powerful! Please check our demo for SUM-shot model. Chat-SUM-shot model is on the way!</span>
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">ðŸš€</span>
          <span class="news-title">Latest Release: <a href="https://github.com/bytedance/Shot2Story" style="color: #007bff;"><u>Video Summarization & Shot Captioning Code</u></a></span>
          <br>
          <span style="font-size: 14px;">Dive into our codes for video summarization and shot captioning, enhancing visual-audio video analysis. Stay tuned for more tasks and codes coming soon!</span>
      </div>
    <!-- </div>
    <div class="container"> -->
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">ðŸŒŸ</span>
          <span class="news-title"> New Release: <a href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md" style="color: #007bff;"><u>20K version of Shot2Story</u></a></span>
          <br>
          <span style="font-size: 14px;">Explore 20k videos with detailed human-annotated summaries; 80k video shots with visual captions, and 40k shots with narration captions for comprehensive audio-visual analysis.</span>
      </div>
        
    </div>
  </section>

  <section class="teaser-section">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          In light of large models, we introduce the Shot2Story benchmark: a multi-shot video resource with detailed textual annotations, ideal for training and validating diverse, temporally distinct video tasks. 
        </h2>
        <!-- <h2 class="subtitle has-text-centered">
          In light of large models, we introduce the 'Shot2Story' benchmark: a multi-shot video resource with detailed annotations, ideal for training and validating diverse, temporally distinct video tasks.
        </h2> -->
        <img src="./static/images/S2S_demo.png" style="width:100%;height:auto;text-align:center;">
        <!-- <h2 class="subtitle has-text-centered">
          <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
          free-viewpoint
          portraits.
        </h2> -->
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              A short clip of video may contain progression of multiple events and an interesting story line. A human needs to capture both the event in every shot and associate them together to understand the story behind it. 
            </p>
            <p>
              In this work, we present a new multi-shot video understanding benchmark Shot2Story with detailed shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. We design several distinct tasks including single-shot video and narration captioning, multi-shot video summarization, and video retrieval with shot descriptions. 
            </p>
            <p>
              Preliminary experiments show some challenges to generate a long and comprehensive video summary.
              Nevertheless, the generated imperfect summaries can already significantly boost the performance of existing video understanding tasks such as video question-answering, promoting an underexplored setting of video understanding with detailed summaries.
            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <!--/ Abstract. -->
<!-- Paper video. -->
<!-- <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Video</h2>
    <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
              frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
  </div>
</div> -->
<!--/ Paper video. -->
      <!-- Contributions. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"><a href="https://huggingface.co/spaces/mhan/Shot2Story"><u>Demo</u></a> for SUM-shot</h2>
          <div class="publication-video" style="display: flex; justify-content: center;">
            <video controls>
              <source src="static/images/gradio_demo_final.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <br>
          <h2 class="title is-3">Shot2Story</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/uhgUrxo1r80?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <br>
          <div class="content has-text-justified">
            <p>
              We provide 20k videos with diverse topics and contents. Each video is annotated with shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. 
            </p>
          </div>
          <br>
          <!-- <br> -->
          <h2 class="title is-4">Dataset glance</h2>
          <div class="content has-text-justified">
            <p>
              The dataset includes an average of 4.0 shots per video, resulting in a total of 80k video shots, each with detailed video caption and narration caption annotations. The average length of our video summaries is 201.8, while the average length of a video is 16s.
            </p>
            <p>
              For more comprehensive details, please refer to the plots below.
            </p>
            <img src="./static/images/dataset_glance.png" style="width:100%;height:auto;text-align:center;">
          </div>
          <br>
          <h2 class="title is-4">Comparison to datasets</h2>
          <div class="content has-text-justified">
            <p>
              High level comparison of our dataset to previous ones. The summary length of ActivityNet and YouCook2 are their combined length of captions in one video. M and G stands for manual and generated, respectively.
            </p>
            <img src="./static/images/datasets_comparison.png" style="width:100%;height:auto;text-align:center;">
          </div>
          <!-- <h2 class="title is-3">What does Shot2Story provide?</h2>
          <div class="content has-text-justified">
            <p>
              We provide 20k videos with diverse topics and contents. Each video is annotated with shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. 
            </p>
          </div> -->
          <!-- </br> -->
          <br>
          <h2 class="title is-3">Baselines and Tasks</h2>
          <div class="content has-text-justified">
            <p>
              We experiment on tasks like single-shot video captioning, narration captioning, video summarization, video shot retrieval and video question-answering. Papers and codes are on the way. Please check for updates!
            </p>
          </div>
          
          <br>
          <h2 class="title is-4">Single-shot captioning - visual and audio</h2>
          <div class="content has-text-justified">
            <p>
              For each single video shot, we experiment on single-shot video captioning and narration captioning. Model structure is shown below. Visual tokens from the CLIP visual backbone and Q-Former (together with a linear layer from MiniGPT4), along with text prompts, form the input to the Vicuna. ASR input is optional for single-shot video captioning.
            </p>
            <img src="./static/images/single_shot_model.png" style="width:100%;height:auto;text-align:center;">
            <p>
              For video captioniong, we try visual singals and visual-audio signals as input. See the results below.
            </p>
            <img src="./static/images/single_shot_video_cap_res.png" style="width:45%; height:auto; display:block; margin-left:auto; margin-right:auto;">
            <p>
              For narration captioniong, we show the results of ASR texts only and our visual-audio model below, which shows the importance of incorporating visual signals.
            </p>
            <img src="./static/images/single_shot_narration_cap_res.png" style="width:56%; height:auto; display:block; margin-left:auto; margin-right:auto;">
          </div>

          <br>
          <h2 class="title is-4">Video summarization - visual and audio</h2>
          <div class="content has-text-justified">
            <p>
              Multi-shot video summarization is a new task that is distinct from existing video description tasks. It requires the model to understand the shot structure of the given video and to provide a coherently paragraph to describe the progression of events in the different shots. 
            </p>
            <p>
              We propose SUM-shot model, as a powerful baseline for multi-shot video analysis. We sample 4 frames in each video shot and prompting the LLM with frame tokens from different shots, as shown below.
            </p>
            <img src="./static/images/sum_shot_model.png" style="width:100%;height:auto;text-align:center;">
            <p>
              We experiment with different models, such as the Video-ChatGPT, SUM-shot holistic (which doesn't have shot information), SUM-text (two-stage approach with LLM tuned) and SUM-shot w/o ASR (which doesn't ASR text as input). Experiment shows that shot information and ASR information are crucial to multi-shot video summarization. Moreover, the task poses a challenge for better training scheme, as indicated by superior performance of SUM-text.
            </p>
            <img src="./static/images/sum_res.png" style="width:67%; height:auto; display:block; margin-left:auto; margin-right:auto;">
            
          </div>
        </div>
      </div>
      <!--/ Contributions. -->


    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{han2023shot2story20k,
          title={Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos}, 
          author={Mingfei Han and Linjie Yang and Xiaojun Chang and Heng Wang},
          journal={arXiv preprint arXiv:2311.17043},
          year={2023}
        }
  </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website design from <a href="https://nerfies.github.io/">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
