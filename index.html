<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos">
  <meta name="keywords"
    content="Multi-shot videos, Video story, Video captioning, Video summarization, Video story generation, VLM, LLM, Large Language Models, Multi-modal learning, Benchmark, Dataset, Instruction tuning, Shot2Story, Videos, HD-VILA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .teaser-section {
        background-color: #f0f0f0; /* Light grey background for contrast */
    }

    .news-item {
      background-color: #f0f8ff; /* Light blue background for contrast */
      border-left: 5px solid #007bff; /* Blue border for emphasis */
      padding: 10px 20px; /* Padding for spacing */
      margin: 10px 0; /* Margin for separation between items */
      font-family: Arial, sans-serif; /* Font styling */
    }

    .news-bug-item {
      background-color: #fffcf0; /* Light blue background for contrast */
      border-left: 5px solid #f7b605; /* Blue border for emphasis */
      padding: 10px 20px; /* Padding for spacing */
      margin: 10px 0; /* Margin for separation between items */
      font-family: Arial, sans-serif; /* Font styling */
    }

    .news-icon {
        color: #ff4500; /* Orange color for the icon */
        font-size: 20px; /* Larger size for the icon */
    }

    .news-title {
        color: #007bff; /* Blue color for the title */
        font-weight: bold; /* Bold font for the title */
        font-size: 18px; /* Font size for the title */
    }

    .news-bug-title {
        color: #f7b605; /* Blue color for the title */
        font-weight: bold; /* Bold font for the title */
        font-size: 18px; /* Font size for the title */
    }

    .news-content {
        color: #333333; /* Dark grey for content text */
        font-size: 14px; /* Font size for content */
        margin-top: 10px; /* Space above the content */
    }

    .publication-videos-container {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      grid-gap: 20px; /* Adjust as needed */
    }
    
    .publication-video {
      position: relative;
      overflow: hidden;
      padding-bottom: 76.25%; /* 16:9 aspect ratio, adjust as needed */
    }
    
    .publication-video iframe {
      position: absolute;
      width: 100%;
      height: 100%;
      top: 0;
      left: 0;
    }
    
    /* Styling for the summary element */
    summary {
      color: #007bff; /* Blue color for the title */
      font-weight: bold; /* Bold font for the title */
      font-size: 18px; /* Font size for the title */
      cursor: pointer; /* Changes cursor to pointer on hover */
      list-style: none; /* Removes default list marker */
      border-left: 5px solid #007bff; /* Blue border for emphasis */
      background-color: #f8f9fa; /* Light gray background */
      padding: 10px 15px; /* Padding for better spacing */
      border-radius: 5px; /* Rounded corners */
      display: block; /* Ensure it fills the container */
  }

    /* Remove default marker for all browsers */
    summary::-webkit-details-marker,
    summary::marker {
      display: none;
    }

    /* Optional: Add custom icon for the collapsible indicator */
    summary::before {
      content: '▶'; /* Triangle pointing right */
      font-size: 12px; /* Smaller font size for the icon */
      color: black; /* Icon color */
      padding-right: 5px; /* Space between icon and text */
      transition: transform 0.2s; /* Smooth transition for icon rotation */
    }

    /* Rotate icon when details is open */
    details[open] summary::before {
      transform: rotate(90deg); /* Rotates the icon 90 degrees */
    }

    
  </style>

</head>

<body>


<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://mingfei.info">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://roomtour3d.github.io">
            RoomTour3D for navigation
          </a>
          <a class="navbar-item" href="https://mingfei.info/shot2story/">
            Shot2Story
          </a>
          <a class="navbar-item" href="https://mingfei.info/PMV/">
            Portrait-mode Videos
          </a>
          <a class="navbar-item" href="https://github.com/ziplab/LongVLM">
            LongVLM for videos
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Shot2Story</h1>
            <h1 class="title is-3 publication-title">A New Benchmark for Comprehensive Understanding of Multi-shot Videos</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://mingfei.info">Mingfei Han (open to job)<sup>2,3,5†*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang<sup>1*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.xiaojun.ai/">Xiaojun Chang<sup>3,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.linayao.com/">Lina Yao<sup>5</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://hengcv.github.io/">Heng Wang<sup>1</sup></a>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal contribution;</span>
              <span class="author-block"><sup>†</sup>Work was done during an internship at Bytedance.</span>
            </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Bytedance Inc.,</span>
              <span class="author-block"><sup>2</sup>ReLER, AAII, University of Technology Sydney,</span>
              <span class="author-block"><sup>3</sup>Department of Computer Vision, MBZUAI</span>
              <span class="author-block"><sup>4</sup>University of Science and Technology of China,</span>
              <span class="author-block"><sup>5</sup>Data61, CSIRO,</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="https://mingfei.info/files/paper_shot2story20k.pdf"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.10300"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/bytedance/Shot2Story/tree/master/code"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/mhan/shot2story"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf-logo.svg" alt="Huggingface logo" width="20" height="20">
                    </span>
                    <span>Annotations</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/mhan/shot2story-videos"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf-logo.svg" alt="Huggingface logo" width="20" height="20">
                    </span>
                    <span>Videos-1</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://1drv.ms/f/s!Ap3OKt6-X52NgXoG4-64N9WZDenS?e=oIHfkZ"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/onedrive-logo.png" alt="Onedrive logo" width="20" height="20">
                    </span>
                    <span>Videos-2</span>
                  </a>
                </span>
                <br>
                <!-- Link to Hugging Face Space -->
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/mhan/Shot2Story"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf-logo.svg" alt="Huggingface logo" width="20" height="20">
                    </span>
                    <span>Chat-Demo</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/mhan/Shot2Story-SUM"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf-logo.svg" alt="Huggingface logo" width="20" height="20">
                    </span>
                    <span>SUM-Demo</span>
                  </a>
                </span>
                <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=uhgUrxo1r80"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Demo-Video</span>
                </a>
              </span>
              <!-- Link to Hugging Face Code -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/your-code-link"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-code"></i>
                  </span>
                  <span>Hugging Face Code</span>
                </a>
              </span> -->

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container">
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">🌟🚀</span>
          <span class="news-title"> Our paper is accepted by ICLR 2025. See you in Singapore.</span>
          <br>
          <span style="font-size: 14px;"> A new version of the paper will be updated soon. </span>
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">🌟</span>
          <span class="news-title"> 5 June Release: <a href="https://huggingface.co/datasets/mhan/shot2story" style="color: #007bff;"><u>Shot2Story-QA benchmark</u></a></span>
          <br>
          <span style="font-size: 14px;">Explore 11K question-answering pairs for benchmarking video-language models on multi-shot understanding. The quality is ensured by human annotation and verification.</span>
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">🚀</span>
          <span class="news-title"> Demo Update: <a href="https://huggingface.co/spaces/mhan/Shot2Story" style="color: #007bff;"><u>ChatBot Demo</u></a> and <a href="https://huggingface.co/spaces/mhan/Shot2Story-SUM" style="color: #007bff;"><u>SumBot Demo</u></a></span>
          <br>
          <span style="font-size: 14px;">In line with our most recent data release, the demo has been updated. Please take a moment to explore our powerful video summarization model.</span>
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">🌟</span>
          <span class="news-title"> 24 April Release: <a href="https://huggingface.co/datasets/mhan/shot2story" style="color: #007bff;"><u>134K version (QA on the way): 43K manual + 90K GPT-V</u></a></span>
          <br>
          <span style="font-size: 14px;">Explore 134k videos with detailed text summaries, consisting of 43K human-annotated and 90k GPTV generated data. Moreover, we release 188K video shots with human-annotated visual captions, and 95K shots with narration captions.</span>
      </div>
      <div class="news-bug-item">
          <span class="news-icon" style="color: #4CAF50;">🌟</span>
          <span class="news-bug-title">Data instruction: Please find our instructions for using and downloading data <a href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md" style="color: #f7b605;"><u>here</u></a>.</span>
          <!-- <br> -->
          <!-- <span style="font-size: 14px;">Detailed and grounded video summaries are powerful! Please check our demo for SUM-shot model. Chat-SUM-shot model is on the way!</span> -->
      </div>
      <div class="news-item">
          <span class="news-icon" style="color: #4CAF50;">🚀</span>
          <span class="news-title">Code Release: <a href="https://github.com/bytedance/Shot2Story" style="color: #007bff;"><u>Video Summarization & Shot Captioning Code</u></a></span>
          <br>
          <span style="font-size: 14px;">Dive into our codes for video summarization and shot captioning, enhancing visual-audio video analysis. Stay tuned for more tasks and codes coming soon!</span>
      </div>
        <details>
        <summary>More news</summary>
          <div class="news-item">
              <span class="news-icon" style="color: #4CAF50;">🌟</span>
              <span class="news-title"> Online demo is back in service: <a href="https://huggingface.co/spaces/mhan/Shot2Story" style="color: #007bff;"><u>HF Space</u></a></span>
              <br>
              <span style="font-size: 14px;">Please feel free to raise the issue on <a href="https://github.com/bytedance/Shot2Story/tree/master/code" style="color: #007bff;"><u>Github</u></a> or <a href="https://huggingface.co/spaces/mhan/Shot2Story/discussions/1" style="color: #007bff;"><u>HF Space</u></a> if the demo fails again.</span>
          </div>
          <div class="news-item">
              <span class="news-icon" style="color: #4CAF50;">🌟</span>
              <span class="news-title"> Multi-shot videos download: <a href="https://huggingface.co/mhan/shot2story-videos" style="color: #007bff;"><u>HF storage</u></a></span>
              <br>
              <span style="font-size: 14px;">In additional to <a href="https://1drv.ms/f/s!Ap3OKt6-X52NgXoG4-64N9WZDenS?e=oIHfkZ">OneDrive storage</a>, we provide additional <a href="https://huggingface.co/mhan/shot2story-videos">Huggingface storage</a> of the multi-shot videos. Please follow the release license.</span>
          </div>
          <div class="news-item">
              <span class="news-icon" style="color: #4CAF50;">🚀</span>
              <span class="news-title">Demo Release: <a href="https://huggingface.co/spaces/mhan/Shot2Story" style="color: #007bff;"><u>SUM-shot model</u></a></span>
              <br>
              <span style="font-size: 14px;">Detailed and grounded video summaries are powerful! Please check our demo for SUM-shot model. Chat-SUM-shot model is on the way!</span>
          </div>
        <!-- </div>
        <div class="container"> -->
          <div class="news-item">
              <span class="news-icon" style="color: #4CAF50;">🌟</span>
              <span class="news-title"> New Release: <a href="https://github.com/bytedance/Shot2Story/blob/master/DATA.md" style="color: #007bff;"><u>20K version of Shot2Story</u></a></span>
              <br>
              <span style="font-size: 14px;">Explore 20k videos with detailed human-annotated summaries; 80k video shots with visual captions, and 40k shots with narration captions for comprehensive audio-visual analysis.</span>
          </div>
        </details>
        
    </div>
  </section>

  <section class="teaser-section">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          In light of large models, we introduce the Shot2Story benchmark: a multi-shot video resource with detailed textual annotations, ideal for training and validating diverse, temporally distinct video tasks. Check the <a href="https://huggingface.co/spaces/mhan/Shot2Story">ChatBot</a> and <a href="https://huggingface.co/spaces/mhan/Shot2Story-SUM">SumBot</a> powered by Shot2Story.
        </h2>
        <div class="publication-videos-container">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/H3HNclyuebg?rel=0&amp;showinfo=0&amp;vq=hd360" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/eh5e0xhRMro?rel=0&amp;showinfo=0&amp;vq=hd360" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              A short clip of video may contain progression of multiple events and an interesting story line. A human need to capture both the event in every shot and associate them together to understand the story behind it. 
            </p>
            <p>
              In this work, we present a new multi-shot video understanding benchmark \dataset with detailed shot-level captions, comprehensive video summaries and question-answering pairs. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. We design several distinct tasks including single-shot video captioning, multi-shot video summarization, and multi-shot video question answering. 
            </p>
            <p>
              Preliminary experiments show some challenges to generate a long and comprehensive video summary for multi-shot videos. Nevertheless, the generated imperfect summaries can already achieve competitive performance on existing video understanding tasks such as video question-answering, promoting an under-explored setting of video understanding with detailed summaries.
            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <!--/ Abstract. -->
<!-- Paper video. -->
<!-- <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Video</h2>
    <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
              frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </div>
  </div>
</div> -->
<!--/ Paper video. -->
      <!-- Contributions. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3"><a href="https://huggingface.co/spaces/mhan/Shot2Story"><u>Demo</u></a> for SUM-shot</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/pEg3l-tDs9M?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div> -->
          <br>
          <h2 class="title is-3">Shot2Story</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/uhgUrxo1r80?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
          <br>
          <div class="content has-text-justified">
            <p>
              We provide 43K videos with diverse topics and contents. Each video is annotated with shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. 
            </p>
          </div>
          <br>
          <!-- <br> -->
          <h2 class="title is-4">Dataset glance</h2>
          <div class="content has-text-justified">
            <img src="./static/images/S2S_demo_wqa.png" style="width:100%;height:auto;text-align:center;">
            <p>
              The dataset includes 43K multi-shot videos, with an average of 4.4 shots per video, resulting in a total of 188k video shots, each with detailed video caption and narration caption annotations. The average length of our video summaries is 218.3, while the average length of a video is 17.1s. In addition, we annotate another 90K multi-shot videos with GPT-V, to facillate better video-language model training.
            </p>
            <img src="./static/images/website_qa_summary_s2s.png" style="width:100%;height:auto;text-align:center;">
            <p>
              Our multi-shot video question-answering benchmark evaluates multi-shot content understanding, temporal ordering, and audio-visual correlation.
            </p>
            <p>
              For more comprehensive details, please refer to the plots below.
            </p>
            <img src="./static/images/stats-new.png" style="width:100%;height:auto;text-align:center;">
          </div>
          <br>
          <h2 class="title is-4">Comparison to datasets</h2>
          <div class="content has-text-justified">
            <p>
              High level comparison of our dataset to previous ones. The summary length of ActivityNet and YouCook2 are their combined length of captions in one video. M and G stands for manual and generated, respectively.
            </p>
            <img src="./static/images/datasets_comparison_new.png" style="width:100%;height:auto;text-align:center;">
          </div>
          <!-- <h2 class="title is-3">What does Shot2Story provide?</h2>
          <div class="content has-text-justified">
            <p>
              We provide 20k videos with diverse topics and contents. Each video is annotated with shot-level captions and comprehensive video summaries. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. 
            </p>
          </div> -->
          <!-- </br> -->
          <br>
          <h2 class="title is-3">Baselines and Tasks</h2>
          <div class="content has-text-justified">
            <p>
              We experiment on tasks like single-shot video captioning, video summarization and video question-answering. Papers and codes are released. We also provide online demos of ChatBot and SumBot. Please have a look at it!
            </p>
          </div>
          
          <br>
          <h2 class="title is-4">Single-shot video captioning - visual and audio</h2>
          <div class="content has-text-justified">
            <p>
              This task involves generating descriptions for individual video shots, where the target description is a concatenation of the visual-only and narration caption for a video shot. This task requires a joint understanding of visual and speech information. Model structure is shown below. Visual tokens from the CLIP visual backbone and Q-Former (together with a linear layer from MiniGPT4), along with text prompts, form the input to the Vicuna. 
            </p>
            <img src="./static/images/single_shot_model.png" style="width:100%;height:auto;text-align:center;">
            <p>
              We experiment VAST with two settings (V+A+S and V+S. V, A and S stand for vision, audio and subtitle respectively), MiniGPT4-C and VideoChat2-C (C stands for captioning). The models are trained on our Shot2Story single-shot video caption data. See the results below.
            </p>
            <img src="./static/images/single_shot_va_cap_res.png" style="width:60%; height:auto; display:block; margin-left:auto; margin-right:auto;">
            
          <br>
          <h2 class="title is-4">Video summarization - visual and audio</h2>
          <div class="content has-text-justified">
            <p>
              Multi-shot video summarization is a new task that is distinct from existing video description tasks. It requires the model to understand the shot structure of the given video and to provide a coherently paragraph to describe the progression of events in the different shots. 
            </p>
            <p>
              We propose SUM-shot model, as a powerful baseline for multi-shot video analysis. We sample 4 frames in each video shot and prompting the LLM with frame tokens from different shots, as shown below.
            </p>
            <img src="./static/images/sum_shot_model.png" style="width:100%;height:auto;text-align:center;">
            <p>
              We experiment with different models, such as the Video-ChatGPT, MiniGPT4-SUM-shot, MiniGPT4-holistic (which doesn't have shot information), SUM-shot w/o ASR (which doesn't ASR text as input), and VideoChat2-SUM-shot (equipped with advanced vision backbone and video pretraining). Experiment shows that shot information and ASR information are crucial to multi-shot video summarization. VideoChat2-SUM-shot further confirms the importance of advanced visual representations.
            </p>
            <img src="./static/images/sum_res_new.png" style="width:77%; height:auto; display:block; margin-left:auto; margin-right:auto;">
          </div>

          <br>
          <h2 class="title is-4">Multi-shot video question-answering</h2>
          <div class="content has-text-justified">
            <p>
              Generated video summaries are supposed to be grounded and detailed, covering rich elements like event progression, holistic topics and audio elements, making them suitable for other vision tasks such as video question-answering. Existing works uses image or video frame captions as input to an LLM to generate question responses. However, little work has been done for the capacity of video summaries. We directly apply our video summarization model on video QA benchmarks, i.e. MSRVTT-QA, ActivityNet-QA and our Shot2Story-QA.
            </p>
            <p>
              Specifically, we first split the testing videos into video shots, and then feed the videos into our SUM-shot models. The generated summaries and the associated questions are then fed into a Vicuna model to derive the answers. We leverage the gpt-3.5-turbo model to generate a binary decision on whether the answer is correct.
            </p>
            <img src="./static/images/s2s_qa_results.png" style="width:95%; height:auto; display:block; margin-left:auto; margin-right:auto;">
            <p>
              we benchmark existing and our proposed video summary models on Shot2Story-QA. Specifically, four popular video-VLMs are compared, i.e., Video-ChatGPT, LLaMA-VID, VideoChat2 and Video-LLaVA. The predicted summaries of MiniGPT4-SUM-shot and VideoChat2-SUM-shot are used to tackle the QA task with the same configuration as zero-shot VQA. Accuracies on temporal-related, multi-shot holistic understanding and audio-related are reported, with the “overall” metric showing the average score from these three sub-tasks. The current video-VLMs present unsatisfying results, potentially due to two factors: (1) The current models does not have audio or ASR as input, lacking capacity with audio-related understanding. (2) Current models do not have training data with detailed descriptions based on multi-shot videos, weakening their performance on holistic understanding and temporal modeling. For our models, VideoChat2-SUM-shot achieves an overall score of 40.5, surpassing the compared models and MiniGPT4-SUM-shot on all three subtasks. This performance underscores the benefits of video pretraining and the advanced visual backbone of VideoChat2. These baseline results highlight the complexities and demanding nature of our Shot2Story-QA task.
            </p>
          </div>
          <br>
          <h2 class="title is-4">Zero-shot video question-answering </h2>
          <div class="content has-text-justified">
            <p>
              As shown in the below table, our results with VideoChat2-SUM-shot surpass 5 out of 6 existing video-VLMs on MSRVTT-QA and 4 out of 6 existing models on ActivityNet-QA. Furthermore, our results are comparable to the SOTA performance on MSRVTT-QA with Video-LLaVA. 
            </p>
            <img src="./static/images/qa_res_new_zr.png" style="width:85%; height:auto; display:block; margin-left:auto; margin-right:auto;">
            <p>
              Note that these models require extensive instruction-tuning data to learn to directly generate answers from visual features and the text prompt, whereas our model bypasses instruction tuning by distilling the video information into a summary. Our model also follows the zero-shot QA setting since the model only uses Shot2Story as training data. Note that MSRVTT contains a large portion of videos with out-of-domain topics such as TV shows and food, while ActivityNet has much longer videos than our training videos. This validates the robustness and transferability of our model across different topics and longer videos. This surprisingly good result indicates that a comprehensive and detailed video summary is a high-quality abstraction of the video,facilitating a wide range of tasks including video QA and video-based conversation
            </p>
          </div>
        </div>
      </div>
      <!--/ Contributions. -->


    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{han2023shot2story20k,
          title={Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos}, 
          author={Mingfei Han and Linjie Yang and Xiaojun Chang and Heng Wang},
          journal={arXiv preprint arXiv:2311.17043},
          year={2023}
        }
  </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website design from <a href="https://nerfies.github.io/">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
